{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's challenge: a sentiment analysis of Trump tweets\n",
    "\n",
    "The goal for today is to introduce Python programming via a real world text analysis task: conducting a sentiment analysis of Twitter data. Our learning objectives include:\n",
    "\n",
    "1. Introducing Python via a real-world text analysis task.\n",
    "2. Illustrating how to load data into (and write data out of) Python\n",
    "3. Learning to implement \"standard\" text pre-processing in Python\n",
    "4. Learning how to conduct a lexicon-based sentiment analysis\n",
    "\n",
    "To achieve these objectives, we will focus on a dataset which contains all of Donald Trump's tweets for the year 2017. Our challenge is to build a simple lexicon (or dictionary) to track negative sentiment in Trump's Twitter feed. Let's get started!\n",
    "\n",
    "#### Note on programming in Python\n",
    "\n",
    "This notebook offers an introduction to progamming in the Python language. It's impossible to cover it all in a single notebook (or a single class!); however, this notebook highlights core aspects of Python that are important for this class. I highly recommend the (free and online!) book <a href=https://python.swaroopch.com/><i>A Byte of Python</i></a> if you would like to further study the ideas outlined in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the required libraries\n",
    "\n",
    "The first step in constructing our sentiment analysis script is load any required libraries using the `import` statement: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tcoan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os # Needed to change your working directory\n",
    "os.chdir('/Users/tcoan/git_repos/notebooks') # CHANGE THIS DIRECTORY!\n",
    "import json # Needed to load the trump_tweets_2017.json file\n",
    "\n",
    "# We will use a couple of regular expressions in this tutorial.\n",
    "# I have another notebook that provides more details on working with\n",
    "# regular expressions.\n",
    "import re\n",
    "\n",
    "# Import NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')     # Download NLTK \"data\" once (see\n",
    "nlkt.download('stopwords') # https://www.nltk.org/data.html)\n",
    "\n",
    "# Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stemming and lemmatization\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Demonstrate pre-built sentiment lexicons\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Import the pandas library using the namespace \"pd\" to \n",
    "# save on typing. We will use pandas for reading/writing data,\n",
    "# as well as for the occasional descriptive statistic\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if there is a library that you need to use that is not pre-installed with Anaconda, you can install it within Jupyter using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read text data into Python for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to read in tweets for our sentiment analysis. The trump_tweets_2017.json is a <a href=\"https://www.w3schools.com/js/js_json_intro.asp\">JSON</a> formatted file and so we need to use the `json` library to open it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The \"opens\" a connection to the trump_tweets data on disk and \"loads\"\n",
    "# into an object called \"tweets\"\n",
    "with open('data/trump_tweets_2017.json', 'r', encoding=\"utf-8\") as jfile:\n",
    "    tweets = json.load(jfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we've \"loaded\" our JSON file, but what is actually stored in <b>variable</b> `tweets`? We could print it, but that doesn't help all that much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that JSON files are read as a list of dictionaries. Awesome, so what's a `list` and whats a `dict`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists\n",
    "\n",
    "A list is just that -- a list of objects. These \"objects\" can be numbers, strings, and even other data structures (such as dictionaries!). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = ['trav', 'dre', 'riley', 'gwen']\n",
    "ages = [41, 42, 10, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trav', 'dre', 'riley', 'gwen']\n"
     ]
    }
   ],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each list will have a <b>length</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can lookup a particular element in the list using the appropriate <b>index</b>. Note that Python indexes lists starting at 0, and moves right to left. So if we wanted to lookup the 2nd element in `names` list, we would type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trav\n"
     ]
    }
   ],
   "source": [
    "print(names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over a list in the opposite direction by using negative indices. So to get the last and second to last name in the list, we could type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gwen\n",
      "riley\n"
     ]
    }
   ],
   "source": [
    "# Last name\n",
    "print(names[-1])\n",
    "\n",
    "# Second to last name\n",
    "print(names[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can `append` new objects to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names.append('ranu')\n",
    "ages.append(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names.insert(0, 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del names[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be using lists a TON in this class, so it's good to get comfortable working with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "In addition to lists, dictionaries are one of the most often used data structures in Python programs. As aptly described in Byte of Python,\n",
    "\n",
    "> \"A dictionary is like an address-book where you can find the address or contact details of a person by knowing only his/her name i.e. we associate keys (name) with values (details). Note that the key must be unique just like you cannot find out the correct information if you have two persons with the exact same name.\"\n",
    "\n",
    "For example, say we wanted to set up a dictionary that maps our names to ages using the data above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_to_age = {'trav': [41, 28], 'dre': 42, 'ranu': 26}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41, 28]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_age['trav']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `name_to_age` dictionary has the following keys: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['trav', 'dre', 'ranu'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_age.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can store our `names` and `ages` list as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names_ages = {'names': names, 'ages': ages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names_ages['ages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like lists, dictonaries are extremely flexible and can store all different kinds of information. We will also use these a TON in this class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's a list of dictionaries?\n",
    "\n",
    "It's exactly what it says---it's a `list` holding 1 or more `dict` objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dre\n"
     ]
    }
   ],
   "source": [
    "name_data = [\n",
    "    {'name': 'trav', 'age': 41},\n",
    "    {'name': 'dre', 'age': 42}\n",
    "]\n",
    "\n",
    "print(name_data[1]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, our `tweets` data that we loaded is just a list of dicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Twitter for iPhone',\n",
       " 'text': 'Jobs are kicking in and companies are coming back to the U.S. Unnecessary regulations and high taxes are being dramatically Cut, and it will only get better. MUCH MORE TO COME!',\n",
       " 'created_at': 'Sat Dec 30 22:42:09 +0000 2017',\n",
       " 'retweet_count': 24332,\n",
       " 'favorite_count': 117013,\n",
       " 'is_retweet': False,\n",
       " 'id_str': '947236393184628741'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'text', 'created_at', 'retweet_count', 'favorite_count', 'is_retweet', 'id_str'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our list of tweets is 2275 long and has the following keys:\n",
      "['source', 'text', 'created_at', 'retweet_count', 'favorite_count', 'is_retweet', 'id_str']\n"
     ]
    }
   ],
   "source": [
    "print(f'Our list of tweets is {len(tweets)} long and has the following keys:\\n{list(tweets[0].keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and writing data with `pandas`\n",
    "\n",
    "We use the `json` library to read our Trump data, but what if your data is saved in something other than JSON? Python has standalone libraries for reading files stored in different formats (e.g., the <a href=https://docs.python.org/3/library/csv.html>csv</a> module). However, the `pandas` library offers a convienent approach to reading and writing files in pretty much any format out there.\n",
    "\n",
    "Note that `pandas` offers so, so much more than just reading and writing files. It's provides an R-like environment for data wrangling and analysis, and is quickly becoming the \"go to\" library for data analysis in Python. For an introduction to `pandas`, please check out:\n",
    "\n",
    "<https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/>\n",
    "\n",
    "However, we won't spend too much time on `pandas` in this section of the course, but instead use it for reading/writing data, as well as a bit of descriptive statistics.\n",
    "\n",
    "Reading data in `pandas` is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Trump tweets CSV into a pandas \"dataframe\"\n",
    "trump_df = pd.read_csv('data/trump_tweets_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trump_df` object is a `pandas` dataframe, which operates similarly to an R data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>9.470000e+17</td>\n",
       "      <td>Jobs are kicking in and companies are coming b...</td>\n",
       "      <td>False</td>\n",
       "      <td>24332</td>\n",
       "      <td>117013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>9.470000e+17</td>\n",
       "      <td>I use Social Media not because I like to, but ...</td>\n",
       "      <td>False</td>\n",
       "      <td>50342</td>\n",
       "      <td>195754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>9.470000e+17</td>\n",
       "      <td>On Taxes: “This is the biggest corporate rate ...</td>\n",
       "      <td>False</td>\n",
       "      <td>16703</td>\n",
       "      <td>73325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>9.470000e+17</td>\n",
       "      <td>Oppressive regimes cannot endure forever, and ...</td>\n",
       "      <td>False</td>\n",
       "      <td>23270</td>\n",
       "      <td>78932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>9.470000e+17</td>\n",
       "      <td>The entire world understands that the good peo...</td>\n",
       "      <td>False</td>\n",
       "      <td>23532</td>\n",
       "      <td>77986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source        id_str  \\\n",
       "0  Twitter for iPhone  9.470000e+17   \n",
       "1  Twitter for iPhone  9.470000e+17   \n",
       "2  Twitter for iPhone  9.470000e+17   \n",
       "3  Twitter for iPhone  9.470000e+17   \n",
       "4  Twitter for iPhone  9.470000e+17   \n",
       "\n",
       "                                                text  is_retweet  \\\n",
       "0  Jobs are kicking in and companies are coming b...       False   \n",
       "1  I use Social Media not because I like to, but ...       False   \n",
       "2  On Taxes: “This is the biggest corporate rate ...       False   \n",
       "3  Oppressive regimes cannot endure forever, and ...       False   \n",
       "4  The entire world understands that the good peo...       False   \n",
       "\n",
       "   retweet_count  favorite_count  \n",
       "0          24332          117013  \n",
       "1          50342          195754  \n",
       "2          16703           73325  \n",
       "3          23270           78932  \n",
       "4          23532           77986  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19480.64043956044"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_df['retweet_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert our `trump_df` to list of dictionaries as above, we need to run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Twitter for iPhone',\n",
       " 'id_str': 9.47e+17,\n",
       " 'text': 'Jobs are kicking in and companies are coming back to the U.S. Unnecessary regulations and high taxes are being dramatically Cut, and it will only get better. MUCH MORE TO COME!',\n",
       " 'is_retweet': False,\n",
       " 'retweet_count': 24332,\n",
       " 'favorite_count': 117013}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump = trump_df.to_dict('records')\n",
    "trump[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">WARNING</span>: The code written below to clean and process our text data assumes that our data is a list of dictionaries. If you try to use this code directly on a `pandas` dataframe it will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df.to_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: prepare text for analysis (aka pre-processing)\n",
    "\n",
    "With our list of tweets in hand, we are now need to get out text ready for analysis. While the process of \"pre-processing\" text will vary a bit based on the specific analysis employed, we will cover the so-called \"standard\" pre-processing procedures:\n",
    "\n",
    "1. Tokenization\n",
    "2. Convert to lower (or upper) case\n",
    "3. Punctuation removal\n",
    "4. Stopword removal\n",
    "\n",
    "Other common pre-processing procedures **not** covered in this notebook include:\n",
    "\n",
    "5. Expanding contractions\n",
    "6. Lemmatizing or stemming\n",
    "7. Removing numbers\n",
    "\n",
    "I will cover these additional pre-processing procedures in a seperate notebook (extended-preprocessing.ipynb) for anyone that is interested.\n",
    "\n",
    "Great, we are ready to get going. Before processing our text, however, we need to get a sense of how Python understands `strings`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings and string methods\n",
    "\n",
    "Python has all of the standard data types (number, strings, bool), but `strings` (or text!) is obviously quite important for text analysis class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jobs are kicking in and companies are coming back to the U.S. Unnecessary regulations and high taxes are being dramatically Cut, and it will only get better. MUCH MORE TO COME!'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweets[0]['text']\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings are \"iterable\", meaning that they have an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also have a set of \"methods\" (or functions) that will be useful for us throughout the course. Here are some of the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lower()` and `upper()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this text needs to be lowercase.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'This text NEEDS to be lowercase.'.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`strip()` white space from the ends of a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We need to strip the white space off of this sentence.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'   We need to strip the white space off of this sentence.         '.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`split()` a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jobs are kicking in and companies are coming back to the U',\n",
       " 'S',\n",
       " ' Unnecessary regulations and high taxes are being dramatically Cut, and it will only get better',\n",
       " ' MUCH MORE TO COME!']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could go on and on here. The point is do yourself a favor and review the following methods:\n",
    "\n",
    "<a href=\"https://www.w3schools.com/python/python_ref_string.asp\">https://www.w3schools.com/python/python_ref_string.asp</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is just the process of splitting up our strings into smaller parts (usually sentences or words). We've already seen one way to do this using the `split()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tweet.split(' ')\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also times when you want to first tokenize a string into sentences and then into words. Here, we can use the `sent_tokenize` function that we imported above to do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jobs are kicking in and companies are coming back to the U.S.', 'Unnecessary regulations and high taxes are being dramatically Cut, and it will only get better.', 'MUCH MORE TO COME!']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(tweet)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jobs are kicking in and companies are coming back to the U.S. Unnecessary regulations and high taxes are being dramatically Cut, and it will only get better. MUCH MORE TO COME!'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our simple twitter sentiment analysis, we will ignore the sentence structure and treat our text as a <b>bag of words</b>. We'll also use the `word_tokenize` function from `nltk` to get a cleaner set of tokens than what was produced using `split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jobs', 'are', 'kicking', 'in', 'and', 'companies', 'are', 'coming', 'back', 'to', 'the', 'U.S', '.', 'Unnecessary', 'regulations', 'and', 'high', 'taxes', 'are', 'being', 'dramatically', 'Cut', ',', 'and', 'it', 'will', 'only', 'get', 'better', '.', 'MUCH', 'MORE', 'TO', 'COME', '!']\n"
     ]
    }
   ],
   "source": [
    "toks = word_tokenize(tweet)\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, so now we know how to tokenize a single tweet. However, we have 2,275 tweets to process. <b>Soluton</b>: the trusty old `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_dict = f'This is {len(tweets)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is 2275'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `for` loop\n",
    "\n",
    "We often want to make repeated calculations and this is where the idea of a \"loop\" comes in. Let's start by taking a look at a `for` loop, which allows you to iterate over a sequence of objects. For example, let's create a list of numbers from 0 to 9 using Python's `range()` function, loop over the list, and print the number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can loop over our list of tokens (i.e., `toks`) and print each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in toks:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use a `for` loop to tokenize our list of tweets? Like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [] # preallocate a list to hold the tokenized tweets\n",
    "for tweet in tweets:\n",
    "    # Note: I'm going to convert everything to lowercase here\n",
    "    # to avoid looping again\n",
    "    tokens.append(word_tokenize(tweet['text'].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We can actually write this code a bit more efficiently by using what is called \"<b>list comprehension</b>\" in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ = [word_tokenize(tweet['text'].lower()) for tweet in tweets]\n",
    "tokens == tokens_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "\n",
    "In many analyses, it makes sense to remove so-called **stopwords**. These are words that show up frequently in a text, but add very little meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# Pull in NLTK's English language stopword list.\n",
    "stops = stopwords.words('english')\n",
    "print(stops[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing to notice is our stopword list assumes lowercase. You always need to check this! But how do we remove these words from our list? Let's to it the long way first and then we will use the equivalent list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stops = [] # preallocate a list to hold tweet-level data\n",
    "# First loop is over tokenized tweets\n",
    "for toks in tokens:\n",
    "    toks_no_stops = [] # preallocate a list to hold token-level data\n",
    "    # Next loop over the individual words in each tokenized tweet\n",
    "    for tok in toks:\n",
    "        # Check if the word (tok) is in the stopword list\n",
    "        if tok not in set(stops):\n",
    "            toks_no_stops.append(tok)\n",
    "    # Store result and move to next tweet\n",
    "    tokens_no_stops.append(toks_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_no_stops[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a more compact version using list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stops = [] # preallocate a list to hold tweet-level data\n",
    "for toks in tokens:\n",
    "    tokens_no_stops.append([tok for tok in toks if tok not in set(stops)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_no_stops[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is still some junk in this tweet (e.g., 'amp', 'https', etc.). It's not uncommon to have \"corpus-specific\" (or \"extended\") stopwords. We can add additional words to the stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'amp', 'https', 'co']\n"
     ]
    }
   ],
   "source": [
    "stops_to_add = ['amp', 'https', 'co']\n",
    "\n",
    "# Concatenate the two lists\n",
    "extended_stops = stops + stops_to_add\n",
    "\n",
    "# The last stopword is now 'co'\n",
    "print(extended_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to use this extended stop word list and we are good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stops = [] # preallocate a list to hold tweet-level data\n",
    "for toks in tokens:\n",
    "    tokens_no_stops.append([tok for tok in toks if tok not in set(extended_stops)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our use of the `word_tokenize()` function makes removing punctuation very, very easy. Check out how `word_tokenize()` handles punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokenize('I love this class!!!!!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, all we need to do is remove strings with a length == 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_punct = []\n",
    "for toks in tokens_no_stops:\n",
    "    tokens_no_punct.append([tok for tok in toks if len(tok) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_punct[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build and use your lexicon (or dictionary)\n",
    "\n",
    "What words should go into our lexicon of \"negative\" words? While there are a number of words with clear, negative conotations, lexicon-based approaches often need to be tailored to a specific task (or at least validated for the specific task at hand). We will start by illustrating how to build and employ our own lexicon -- as this could be useful across a range of tasks, not just sentiment analysis -- and then I will quickly demonstate how to use existing sentiment lexicons in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting simple: looking up a single word\n",
    "\n",
    "Let's start simple and examine the presence of absence of a single word in our corpus: fake. Here's one way of completing this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = []\n",
    "for toks in tokens_no_punct:\n",
    "    if 'fake' in toks:\n",
    "        fake.append(1)\n",
    "    else:\n",
    "        fake.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our negative sentiment lexicon\n",
    "\n",
    "A lexicon (or dictonary) is just a list of words. That's it! So we can start by created list of \"negative\" words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = ['fake','hoax', 'idiot', 'moron', 'phony', 'fight', 'dishonest', 'unfair']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Okay, okay, this list isn't exhaustive, but it is good enough to show you how to implement your own lexicon-based approach. Using our `negative` words lexicon takes a little code, but we have all of the tools to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by looping over each list of tokens\n",
    "negative_sentiment_score = []\n",
    "for toks in tokens_no_punct:\n",
    "    # Define a counter to hold the number of negative words\n",
    "    negative_words = 0\n",
    "    # It is possible for a tweet not to have text. If this is the case,\n",
    "    # we need to skip it.\n",
    "    if len(toks) == 0:\n",
    "        print(\"This tweet does not have text. Setting negative sentiment to 0.\")\n",
    "        negative_sentiment_score.append(0) \n",
    "    else:\n",
    "        # Then loop over each token in each list\n",
    "        for tok in toks:\n",
    "            if tok in set(negative):\n",
    "                negative_words += 1\n",
    "        negative_sentiment_score.append(negative_words/len(toks)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sentiment_score[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-built sentiment lexicons\n",
    "\n",
    "There are a number of good sentiment lexicons already available and `NLTK` provides access to a number of these (see the <a href=\"https://www.nltk.org/api/nltk.sentiment.html\">nltk.sentiment api documentation</a> for a full list). Once lexicon that has been shown to work well across a range of tasks is <a href=\"http://scholar.google.co.uk/scholar_url?url=https://ojs.aaai.org/index.php/ICWSM/article/download/14550/14399&hl=en&sa=X&ei=rUoRYKXUDIfCmgGXoLToAw&scisig=AAGBfm22NmYm4wMvPAvkQZuGz-24V1Wu1A&nossl=1&oi=scholarr\">VADER</a>. Here's how you can use VADER in `NLTK`. First, start by instantiating the `SentimentIntensityAnalyzer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader.polarity_scores(\"This class really does suck.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no pre-processing necessary -- you can simply pass your text in \"raw\" form. Applying to our Trump example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    # Here I'm adding a new field to the \"tweets\" dictionary directly\n",
    "    # While this changes the original dictionry (and so requires caution),\n",
    "    # it ensures that we have all of the relevant meta-data about the tweet\n",
    "    # easily accessble.\n",
    "    tweet['negative_sentiment'] = vader.polarity_scores(tweet['text'])['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus material: defining a pre-processing function\n",
    "\n",
    "It's good practice to roll up our various bits of pre-processing in a single **function**. Functions allow to reuse pieces (or blocks) of code. We do so by declaring a function using the `def` statement. We have already used several of Python's built-in functions earlier in this tutoral. For instance, we \"called\" the `len` function to get the number of characters in a string. Python, however, makes it super easy to define your own functions.\n",
    "\n",
    "For example, we can combine the main pre-processing steps above into a single function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet, stops):\n",
    "    '''\n",
    "    Helper function to pre-process tweet text. It takes an individual tweet,\n",
    "    tokenizes it, converts to lowers, and removes punctuation.\n",
    "    \n",
    "    Args:\n",
    "        tweet (str): The (unprocessed) tweet text\n",
    "        stops (list): Stopword list to use\n",
    "    \n",
    "    Returns:\n",
    "        list of lists: Returns processed tokens\n",
    "    '''\n",
    "    # Convert the entire tweet to lowercase, tokenize, and remove stop words\n",
    "    tokens = [token for token in word_tokenize(tweet.lower()) \n",
    "          if token not in set(stops)]\n",
    "    # Remove punctuation\n",
    "    tokens_no_punct = [token for token in tokens if len(token) > 1]\n",
    "    # Remove numbers and return the processed list of tokens\n",
    "    return tokens_no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then process our corpus using a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean = [process_tweet(tweet['text'], extended_stops) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_clean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set up a function to use our lexicon in the same exact way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lexicon (tokens, lexicon):\n",
    "    '''\n",
    "    Takes a tokenized set of texts and counts the number of tokens\n",
    "    in each text included in a lexicon.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list of list): Tokenized text\n",
    "        lexicon (list of str): List of tokens to lookup\n",
    "    '''\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        lexicon_words = 0\n",
    "        if len(token) == 0:\n",
    "            result.append(None) \n",
    "        else:\n",
    "            # Then loop over each token in each list\n",
    "            for tok in token:\n",
    "                if tok in set(lexicon):\n",
    "                    lexicon_words += 1\n",
    "            result.append(lexicon_words/len(toks)) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calculate_lexicon(tweets_clean, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.14285714285714285,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05714285714285714,\n",
       " 0.0]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

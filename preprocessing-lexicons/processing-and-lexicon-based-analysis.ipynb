{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's challenge: a sentiment analysis of Trump tweets\n",
    "\n",
    "The goal for today is to introduce Python programming via a real world text analysis task: conducting a sentiment analysis of Twitter data. Our learning objectives include:\n",
    "\n",
    "1. Introducing Python via a real-world text analysis task.\n",
    "2. Illustrating how to load data into (and write data out of) Python\n",
    "3. Learning to implement \"standard\" text pre-processing in Python\n",
    "4. Learning how to conduct a lexicon-based sentiment analysis\n",
    "\n",
    "To achieve these objectives, we will focus on a dataset which contains all of Donald Trump's tweets for the year 2017. Our challenge is to build a simple lexicon (or dictionary) to track negative sentiment in Trump's Twitter feed. Let's get started!\n",
    "\n",
    "#### Note on programming in Python\n",
    "\n",
    "This notebook offers an introduction to progamming in the Python language. It's impossible to cover it all in a single notebook (or a single class!); however, this notebook highlights core aspects of Python that are important for this class. I highly recommend the (free and online!) book <a href=https://python.swaroopch.com/><i>A Byte of Python</i></a> if you would like to further study the ideas outlined in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the required libraries\n",
    "\n",
    "The first step in constructing our sentiment analysis script is load any required libraries using the `import` statement: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os # Needed to change your working directory\n",
    "os.chdir('/Users/tcoan/git_repos/notebooks') # CHANGE THIS DIRECTORY!\n",
    "import json # Needed to load the trump_tweets_2017.json file\n",
    "\n",
    "# We will use a couple of regular expressions in this tutorial.\n",
    "# I have another notebook that provides more details on working with\n",
    "# regular expressions.\n",
    "import re\n",
    "\n",
    "# Import NLTK\n",
    "import nltk\n",
    "# nltk.download() # Download NLTK \"data\" once (see\n",
    "                  # https://www.nltk.org/data.html)\n",
    "\n",
    "# Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stemming and lemmatization\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Demonstrate pre-built sentiment lexicons\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if there is a library that you need to use that is not pre-installed with Anaconda, you can install it within Jupyter using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read text data into Python for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to read in tweets for our sentiment analysis. The trump_tweets_2017.json is a <a href=\"https://www.w3schools.com/js/js_json_intro.asp\">JSON</a> formatted file and so we need to use the `json` library to open it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The \"opens\" a connection to the trump_tweets data on disk and \"loads\"\n",
    "# into an object called \"tweets\"\n",
    "with open('data/trump_tweets_2017.json', 'r', encoding=\"utf-8\") as jfile:\n",
    "    tweets = json.load(jfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we've \"loaded\" our JSON file, but what is actually stored in <b>variable</b> `tweets`? We could print it, but that doesn't help all that much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that JSON files are read as a list of dictionaries. Awesome, so what's a `list` and whats a `dict`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists\n",
    "\n",
    "A list is just that -- a list of objects. These \"objects\" can be numbers, strings, and even other data structures (such as dictionaries!). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = ['trav', 'dre', 'riley', 'gwen']\n",
    "ages = [41, 42, 10, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each list will have a <b>length</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can lookup a particular element in the list using the appropriate <b>index</b>. Note that Python indexes lists starting at 0, and moves right to left. So if we wanted to lookup the 2nd element in `names` list, we would type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over a list in the opposite direction by using negative indices. So to get the last and second to last name in the list, we could type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Last name\n",
    "print(names[-1])\n",
    "\n",
    "# Second to last name\n",
    "print(names[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can `append` new objects to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names.append('ranu')\n",
    "ages.append(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be using lists a TON in this class, so it's good to get comfortable working with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "In addition to lists, dictionaries are one of the most often used data structures in Python programs. As aptly described in Byte of Python,\n",
    "\n",
    "> \"A dictionary is like an address-book where you can find the address or contact details of a person by knowing only his/her name i.e. we associate keys (name) with values (details). Note that the key must be unique just like you cannot find out the correct information if you have two persons with the exact same name.\"\n",
    "\n",
    "For example, say we wanted to set up a dictionary that maps our names to ages using the data above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_to_age = {'trav': 41, 'dre': 42, 'ranu': 26}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_age['trav']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `name_to_age` dictionary has the following keys: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['trav', 'dre', 'ranu'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_age.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can store our `names` and `ages` list as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names_ages = {'names': names, 'ages': ages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 42, 10, 4, 26]\n"
     ]
    }
   ],
   "source": [
    "print(names_ages['ages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like lists, dictonaries are extremely flexible and can store all different kinds of information. We will also use these a TON in this class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's a list of dictionaries?\n",
    "\n",
    "It's exactly what it says---it's a `list` holding 1 or more `dict` objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dre\n"
     ]
    }
   ],
   "source": [
    "name_data = [\n",
    "    {'name': 'trav', 'age': 41},\n",
    "    {'name': 'dre', 'age': 42}\n",
    "]\n",
    "\n",
    "print(name_data[1]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, our `tweets` data that we loaded is just a list of dicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Twitter for iPhone',\n",
       " 'text': 'Jobs are kicking in and companies are coming back to the U.S. Unnecessary regulations and high taxes are being dramatically Cut, and it will only get better. MUCH MORE TO COME!',\n",
       " 'created_at': 'Sat Dec 30 22:42:09 +0000 2017',\n",
       " 'retweet_count': 24332,\n",
       " 'favorite_count': 117013,\n",
       " 'is_retweet': False,\n",
       " 'id_str': '947236393184628741'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our list of tweets is 2275 long and has the following keys:\n",
      "['source', 'text', 'created_at', 'retweet_count', 'favorite_count', 'is_retweet', 'id_str']\n"
     ]
    }
   ],
   "source": [
    "print(f'Our list of tweets is {len(tweets)} long and has the following keys:\\n{list(tweets[0].keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and writing data with `pandas`\n",
    "\n",
    "We use the `json` library to read our Trump data, but what if your data is saved in something other than JSON? Python has standalone libraries for reading files stored in different formats (e.g., the <a href=https://docs.python.org/3/library/csv.html>csv</a> module). However, the `pandas` library offers a convienent approach to reading and writing files in pretty much any format out there.\n",
    "\n",
    "Note that `pandas` offers so, so much more than just reading and writing files. It's provides an R-like environment for data wrangling and analysis, and is quickly becoming the \"go to\" library for data scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: prepare text for analysis (aka pre-processing)\n",
    "\n",
    "With our list of tweets in hand, we are now need to get out text ready for analysis. While the process of \"pre-processing\" text will vary a bit based on the specific analysis employed, so-called \"standard\" pre-processing includes some combination of the following procedures:\n",
    "\n",
    "1. Tokenization\n",
    "2. Convert to lower (or upper) case\n",
    "3. Expanding contractions\n",
    "4. Punctuation removal\n",
    "5. Stopword removal\n",
    "6. Removing numbers\n",
    "7. Lematization (or stemming)\n",
    "\n",
    "Before processing our text, however, we need to get a sense of how Python understands `strings`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings and string methods\n",
    "\n",
    "Python has all of the standard data types (number, strings, bool), but `strings` (or text!) is obviously quite important for text analysis class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jobs are kicking in and companies are coming back to the U.S. Unnecessary regulations and high taxes are being dramatically Cut, and it will only get better. MUCH MORE TO COME!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweets[0]['text']\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings are \"iterable\", meaning that they have an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also have a set of \"methods\" (or functions) that will be useful for us throughout the course. Here are some of the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lower()` and `upper()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this text needs to be lowercase.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'This text NEEDS to be lowercase.'.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`strip()` white space from the ends of a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We need to strip the white space off of this sentence.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'   We need to strip the white space off of this sentence.         '.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`split()` a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jobs',\n",
       " 'are',\n",
       " 'kicking',\n",
       " 'in',\n",
       " 'and',\n",
       " 'companies',\n",
       " 'are',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'to',\n",
       " 'the',\n",
       " 'U.S.',\n",
       " 'Unnecessary',\n",
       " 'regulations',\n",
       " 'and',\n",
       " 'high',\n",
       " 'taxes',\n",
       " 'are',\n",
       " 'being',\n",
       " 'dramatically',\n",
       " 'Cut,',\n",
       " 'and',\n",
       " 'it',\n",
       " 'will',\n",
       " 'only',\n",
       " 'get',\n",
       " 'better.',\n",
       " 'MUCH',\n",
       " 'MORE',\n",
       " 'TO',\n",
       " 'COME!']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could go on and on here. The point is do yourself a favor and review the following methods:\n",
    "\n",
    "<a href=\"https://www.w3schools.com/python/python_ref_string.asp\">https://www.w3schools.com/python/python_ref_string.asp</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is just the process of splitting up our strings into smaller parts (usually sentences or words). We've already seen one way to do this using the `split()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jobs', 'are', 'kicking', 'in', 'and', 'companies', 'are', 'coming', 'back', 'to', 'the', 'U.S.', 'Unnecessary', 'regulations', 'and', 'high', 'taxes', 'are', 'being', 'dramatically', 'Cut,', 'and', 'it', 'will', 'only', 'get', 'better.', 'MUCH', 'MORE', 'TO', 'COME!']\n"
     ]
    }
   ],
   "source": [
    "toks = tweet.split(' ')\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also times when you want to first tokenize a string into sentences and then into words. Here, we can use the `sent_tokenize` function that we imported above to do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jobs are kicking in and companies are coming back to the U.S.', 'Unnecessary regulations and high taxes are being dramatically Cut, and it will only get better.', 'MUCH MORE TO COME!']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(tweet)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our simple twitter sentiment analysis, we will ignore the sentence structure and treat our text as a <b>bag of words</b>. We'll also use the `word_tokenize` function from `nltk` to get a cleaner set of tokens than what was produced using `split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jobs', 'are', 'kicking', 'in', 'and', 'companies', 'are', 'coming', 'back', 'to', 'the', 'U.S', '.', 'Unnecessary', 'regulations', 'and', 'high', 'taxes', 'are', 'being', 'dramatically', 'Cut', ',', 'and', 'it', 'will', 'only', 'get', 'better', '.', 'MUCH', 'MORE', 'TO', 'COME', '!']\n"
     ]
    }
   ],
   "source": [
    "toks = word_tokenize(tweet)\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, so now we know how to tokenize a single tweet. However, we have 2,275 tweets to process. <b>Soluton</b>: the trusty old `for` loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `for` loop\n",
    "\n",
    "We often want to make repeated calculations and this is where the idea of a \"loop\" comes in. Let's start by taking a look at a `for` loop, which allows you to iterate over a sequence of objects. For example, let's create a list of numbers from 0 to 9 using Python's `range()` function, loop over the list, and print the number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can loop over our list of tokens (i.e., `toks`) and print each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs\n",
      "are\n",
      "kicking\n",
      "in\n",
      "and\n",
      "companies\n",
      "are\n",
      "coming\n",
      "back\n",
      "to\n",
      "the\n",
      "U.S\n",
      ".\n",
      "Unnecessary\n",
      "regulations\n",
      "and\n",
      "high\n",
      "taxes\n",
      "are\n",
      "being\n",
      "dramatically\n",
      "Cut\n",
      ",\n",
      "and\n",
      "it\n",
      "will\n",
      "only\n",
      "get\n",
      "better\n",
      ".\n",
      "MUCH\n",
      "MORE\n",
      "TO\n",
      "COME\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in toks:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use a `for` loop to tokenize our list of tweets? Like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [] # preallocate a list to hold the tokenized tweets\n",
    "for tweet in tweets:\n",
    "    # Note: I'm going to convert everything to lowercase here\n",
    "    # to avoid looping again\n",
    "    tokens.append(word_tokenize(tweet['text'].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'use', 'social', 'media', 'not', 'because', 'i', 'like', 'to', ',', 'but', 'because', 'it', 'is', 'the', 'only', 'way', 'to', 'fight', 'a', 'very', 'dishonest', 'and', 'unfair', '“', 'press', ',', '”', 'now', 'often', 'referred', 'to', 'as', 'fake', 'news', 'media', '.', 'phony', 'and', 'non-existent', '“', 'sources', '”', 'are', 'being', 'used', 'more', 'often', 'than', 'ever', '.', 'many', 'stories', '&', 'amp', ';', 'reports', 'a', 'pure', 'fiction', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We can actually write this code a bit more efficiently by using what is called \"<b>list comprehension</b>\" in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_ = [word_tokenize(tweet['text'].lower()) for tweet in tweets]\n",
    "tokens == tokens_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "\n",
    "In many analyses, it makes sense to remove so-called **stopwords**. These are words that show up frequently in a text, but add very little meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# Pull in NLTK's English language stopword list\n",
    "stops = stopwords.words('english')\n",
    "print(stops[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing to notice is our stopword list assumes lowercase. You always need to check this! But how do we remove these words from our list? Let's to it the long way first and then we will use the equivalent list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stops = [] # preallocate a list to hold tweet-level data\n",
    "# First loop is over tokenized tweets\n",
    "for toks in tokens:\n",
    "    toks_no_stops = [] # preallocate a list to hold token-level data\n",
    "    # Next loop over the individual words in each tokenized tweet\n",
    "    for tok in toks:\n",
    "        # Check if the word (tok) is in the stopword list\n",
    "        if tok not in set(stops):\n",
    "            toks_no_stops.append(tok)\n",
    "    # Store result and move to next tweet\n",
    "    tokens_no_stops.append(toks_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jobs', 'kicking', 'companies', 'coming', 'back', 'u.s.', 'unnecessary', 'regulations', 'high', 'taxes', 'dramatically', 'cut', ',', 'get', 'better', '.', 'much', 'come', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_no_stops[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a more compact version using list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stops = [] # preallocate a list to hold tweet-level data\n",
    "for toks in tokens:\n",
    "    tokens_no_stops.append([tok for tok in toks if tok not in set(stops)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jobs', 'kicking', 'companies', 'coming', 'back', 'u.s.', 'unnecessary', 'regulations', 'high', 'taxes', 'dramatically', 'cut', ',', 'get', 'better', '.', 'much', 'come', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_no_stops[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our use of the `word_tokenize()` function makes removing punctuation very, very easy. Check out how `word_tokenize()` handles punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'this', 'class', '!', '!', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize('I love this class!!!!!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, all we need to do is remove strings with a length == 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_punct = []\n",
    "for toks in tokens_no_stops:\n",
    "    tokens_no_punct.append([tok for tok in toks if len(tok) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jobs',\n",
       " 'kicking',\n",
       " 'companies',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'u.s.',\n",
       " 'unnecessary',\n",
       " 'regulations',\n",
       " 'high',\n",
       " 'taxes',\n",
       " 'dramatically',\n",
       " 'cut',\n",
       " 'get',\n",
       " 'better',\n",
       " 'much',\n",
       " 'come']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_no_punct[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build and use your lexicon (or dictionary)\n",
    "\n",
    "What words should go into our lexicon of \"negative\" words? While there are a number of words with clear, negative conotations, lexicon-based approaches often need to be tailored to a specific task (or at least validated for the specific task at hand). We will start by illustrating how to build and employ our own lexicon -- as this could be useful across a range of tasks, not just sentiment analysis -- and then I will quickly demonstate how to use existing sentiment lexicons in Python.\n",
    "\n",
    "A lexicon (or dictonary) is just a list of words. That's it! So we can start by created list of \"negative\" words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = ['fake','hoax', 'idiot', 'moron', 'phony', 'fight', 'dishonest', 'unfair']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Okay, okay, this list isn't exhaustive, but it is good enough to show you how to implement your own lexicon-based approach. Using our `negative` words lexicon takes a little code, but we have all of the tools to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tweet does not have text. Setting negative sentiment to 0.\n"
     ]
    }
   ],
   "source": [
    "# Start by looping over each list of tokens\n",
    "negative_sentiment_score = []\n",
    "for toks in tokens_no_punct:\n",
    "    # Define a counter to hold the number of negative words\n",
    "    negative_words = 0\n",
    "    # It is possible for a tweet not to have text. If this is the case,\n",
    "    # we need to skip it.\n",
    "    if len(toks) == 0:\n",
    "        print(\"This tweet does not have text. Setting negative sentiment to 0.\")\n",
    "        negative_sentiment_score.append(0) \n",
    "    else:\n",
    "        # Then loop over each token in each list\n",
    "        for tok in toks:\n",
    "            if tok in set(negative):\n",
    "                negative_words += 1\n",
    "        negative_sentiment_score.append(negative_words/len(toks)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.19230769230769232,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06666666666666667,\n",
       " 0.0]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_sentiment_score[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-built sentiment lexicons\n",
    "\n",
    "There are a number of good sentiment lexicons already available and `NLTK` provides access to a number of these (see the <a href=\"https://www.nltk.org/api/nltk.sentiment.html\">nltk.sentiment api documentation</a> for a full list). Once lexicon that has been shown to work well across a range of tasks is <a href=\"http://scholar.google.co.uk/scholar_url?url=https://ojs.aaai.org/index.php/ICWSM/article/download/14550/14399&hl=en&sa=X&ei=rUoRYKXUDIfCmgGXoLToAw&scisig=AAGBfm22NmYm4wMvPAvkQZuGz-24V1Wu1A&nossl=1&oi=scholarr\">VADER</a>. Here's how you can use VADER in `NLTK`. First, start by instantiating the `SentimentIntensityAnalyzer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.443, 'neu': 0.557, 'pos': 0.0, 'compound': -0.4902}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.polarity_scores(\"This class really does suck.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no pre-processing necessary -- you can simply pass your text in \"raw\" form. Applying to our Trump example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    # Here I'm adding a new field to the \"tweets\" dictionary directly\n",
    "    # While this changes the original dictionry (and so requires caution),\n",
    "    # it ensures that we have all of the relevant meta-data about the tweet\n",
    "    # easily accessble.\n",
    "    tweet['negative_sentiment'] = vader.polarity_scores(tweet['text'])['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'Twitter for iPhone', 'text': 'I use Social Media not because I like to, but because it is the only way to fight a VERY dishonest and unfair “press,” now often referred to as Fake News Media. Phony and non-existent “sources” are being used more often than ever. Many stories &amp; reports a pure fiction!', 'created_at': 'Sat Dec 30 22:36:41 +0000 2017', 'retweet_count': 50342, 'favorite_count': 195754, 'is_retweet': False, 'id_str': '947235015343202304', 'negative_sentiment': 0.344}\n"
     ]
    }
   ],
   "source": [
    "print(tweets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
